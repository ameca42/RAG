"""
æ›´æ–°æ–‡ç« å‘é‡å…¥åº“è„šæœ¬ - æ”¯æŒå¼ºåˆ¶æ›´æ–°å·²æœ‰æ–‡ç« çš„å‘é‡æ•°æ®ã€‚
"""

import sys
import os
import json
import argparse
from typing import Dict, Any

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.chains.document_processor import DocumentProcessor
from app.chains.vector_pipeline import VectorPipeline
from app.core.logger import logger


class UpdateVectorPipeline(VectorPipeline):
    """æ‰©å±•çš„å‘é‡ç®¡é“ï¼Œæ”¯æŒå¼ºåˆ¶æ›´æ–°ã€‚"""

    def __init__(self, collection_name: str = "hacker_news"):
        """
        åˆå§‹åŒ–æ›´æ–°å‘é‡ç®¡é“ã€‚

        Args:
            collection_name: ChromaDB é›†åˆåç§°
        """
        super().__init__(collection_name)
        self.force_update = False

    def set_force_update(self, force: bool = True):
        """è®¾ç½®æ˜¯å¦å¼ºåˆ¶æ›´æ–°å·²æœ‰æ–‡ç« ã€‚"""
        self.force_update = force
        logger.info(f"Force update mode: {'enabled' if force else 'disabled'}")

    def ingest_article(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¼ºåˆ¶æ›´æ–°ç‰ˆæœ¬çš„æ–‡ç« å…¥åº“ã€‚

        Args:
            article: Article dictionary from storage

        Returns:
            Dictionary with ingestion stats
        """
        item_id = article.get("item_id")
        title = article.get("title", "")

        if not item_id:
            logger.error("Article missing item_id, skipping ingestion")
            return {"error": "missing_item_id", "ingested": 0}

        # åœ¨å¼ºåˆ¶æ›´æ–°æ¨¡å¼ä¸‹ï¼Œå…ˆåˆ é™¤å·²æœ‰çš„æ–‡æ¡£
        if self.force_update:
            self._remove_existing_documents(item_id)
        else:
            # æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨ï¼ˆåŸç‰ˆé€»è¾‘ï¼‰
            if self.vector_store.check_exists(item_id, "article"):
                logger.info(f"Article {item_id} already exists in vector store, skipping (use --force to update)")
                return {"status": "skipped", "ingested": 0, "item_id": item_id}

        try:
            # å¤„ç†æ–‡ç« ä¸ºæ–‡æ¡£
            documents = self.doc_processor.process_article(article)

            if not documents:
                logger.warning(f"No documents created for article '{title[:50]}...'")
                return {"status": "no_documents", "ingested": 0, "item_id": item_id}

            # æ·»åŠ åˆ°å‘é‡åº“
            doc_ids = self.vector_store.add_documents(documents)

            action = "updated" if self.force_update else "ingested"
            logger.success(
                f"Successfully {action} article '{title[:50]}...' "
                f"({len(documents)} documents, {len(doc_ids)} IDs)"
            )

            return {
                "status": "success",
                "ingested": len(documents),
                "item_id": item_id,
                "doc_ids": doc_ids,
                "updated": self.force_update
            }

        except Exception as e:
            logger.error(f"Error processing article {item_id}: {e}")
            return {"error": str(e), "ingested": 0, "item_id": item_id}

    def _remove_existing_documents(self, item_id: str):
        """
        åˆ é™¤æŒ‡å®šæ–‡ç« çš„æ‰€æœ‰æ–‡æ¡£ã€‚

        Args:
            item_id: æ–‡ç« ID
        """
        try:
            # è·å–æ‰€æœ‰åŒ¹é…çš„æ–‡æ¡£
            results = self.vector_store.similarity_search(
                query="",  # ç©ºæŸ¥è¯¢ï¼Œé€šè¿‡metadataè¿‡æ»¤
                k=1000,  # è·å–å¤§é‡ç»“æœ
                filter_dict={"item_id": item_id}
            )

            if results:
                # æå–æ–‡æ¡£ID
                doc_ids = [doc.metadata.get("doc_id") for doc in results if doc.metadata.get("doc_id")]

                if doc_ids:
                    # åˆ é™¤æ–‡æ¡£
                    self.vector_store.collection.delete(ids=doc_ids)
                    logger.info(f"Removed {len(doc_ids)} existing documents for article {item_id}")
            else:
                logger.info(f"No existing documents found for article {item_id}")

        except Exception as e:
            logger.warning(f"Error removing existing documents for article {item_id}: {e}")

    def update_batch(self, articles: list, force: bool = False) -> Dict[str, Any]:
        """
        æ‰¹é‡æ›´æ–°æ–‡ç« åˆ°å‘é‡åº“ã€‚

        Args:
            articles: æ–‡ç« åˆ—è¡¨
            force: æ˜¯å¦å¼ºåˆ¶æ›´æ–°å·²æœ‰æ–‡ç« 

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        self.force_update = force

        if not articles:
            logger.warning("No articles to update")
            return {"total": 0, "updated": 0, "skipped": 0, "errors": 0}

        stats = {
            "total": len(articles),
            "updated": 0,
            "skipped": 0,
            "errors": 0,
            "docs_created": 0
        }

        action_word = "updating" if force else "ingesting"
        logger.info(f"Starting batch {action_word} of {len(articles)} articles")

        for article in articles:
            try:
                result = self.ingest_article(article)

                if result["status"] == "success":
                    if result.get("updated"):
                        stats["updated"] += 1
                    else:
                        stats["updated"] += 1  # æ–°æ–‡ç« ä¹Ÿç®—æ›´æ–°
                    stats["docs_created"] += result["ingested"]
                elif result["status"] == "skipped":
                    stats["skipped"] += 1
                else:
                    stats["errors"] += 1

            except Exception as e:
                logger.error(f"Error processing article: {e}")
                stats["errors"] += 1
                continue

        logger.success(
            f"Batch update complete: {stats['updated']} updated, "
            f"{stats['skipped']} skipped, {stats['errors']} errors, "
            f"{stats['docs_created']} docs created"
        )

        return stats


def main():
    """ä¸»å‡½æ•°ï¼šæ‰¹é‡æ›´æ–°æ–‡ç« åˆ°å‘é‡åº“ã€‚"""
    parser = argparse.ArgumentParser(description="æ›´æ–°æ–‡ç« å‘é‡æ•°æ®")
    parser.add_argument("--article-id", type=int, help="åªæ›´æ–°æŒ‡å®šIDçš„æ–‡ç« ")
    parser.add_argument("--topic", type=str, help="åªæ›´æ–°æŒ‡å®šè¯é¢˜çš„æ–‡ç« ")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶æ›´æ–°å·²æœ‰æ–‡ç« ")
    parser.add_argument("--recent", type=int, help="åªæ›´æ–°æœ€è¿‘Nç¯‡æ–‡ç« ")

    args = parser.parse_args()

    print("\n" + "=" * 60)
    print("ğŸ”„ æ–‡ç« å‘é‡æ›´æ–°è„šæœ¬")
    print("=" * 60)

    # 1. åŠ è½½æ–‡ç« æ•°æ®
    articles_file = "data/articles.json"

    if not os.path.exists(articles_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {articles_file}")
        print("è¯·å…ˆè¿è¡Œçˆ¬è™«: python -m app.crawler.crawler -n 30")
        return

    print(f"\n1. åŠ è½½æ–‡ç« æ•°æ®...")
    with open(articles_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    articles = data.get("articles", [])
    print(f"   æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

    # 2. ç­›é€‰æ–‡ç« 
    if args.article_id:
        articles = [a for a in articles if a.get("item_id") == args.article_id]
        print(f"   ç­›é€‰åˆ° {len(articles)} ç¯‡æ–‡ç«  (ID: {args.article_id})")
    elif args.topic:
        articles = [a for a in articles if a.get("topic") == args.topic]
        print(f"   ç­›é€‰åˆ° {len(articles)} ç¯‡æ–‡ç«  (è¯é¢˜: {args.topic})")
    elif args.recent:
        # æŒ‰åˆ†æ•°æ’åºï¼Œå–æœ€è¿‘çš„Nç¯‡
        articles = sorted(articles, key=lambda x: x.get("score", 0), reverse=True)[:args.recent]
        print(f"   ç­›é€‰åˆ° {len(articles)} ç¯‡æ–‡ç«  (æœ€è¿‘ {args.recent} ç¯‡)")

    if not articles:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
        return

    # 3. åˆå§‹åŒ–å‘é‡ç®¡é“
    print(f"\n2. åˆå§‹åŒ–å‘é‡ç®¡é“...")
    print(f"   å¼ºåˆ¶æ›´æ–°æ¨¡å¼: {'å¯ç”¨' if args.force else 'ç¦ç”¨'}")

    pipeline = UpdateVectorPipeline()
    pipeline.set_force_update(args.force)

    # 4. æ‰¹é‡æ›´æ–°
    print(f"\n3. å¼€å§‹æ‰¹é‡æ›´æ–°...")
    print(f"   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")

    try:
        result = pipeline.update_batch(articles, force=args.force)

        print(f"\nâœ… æ›´æ–°å®Œæˆï¼")
        print(f"=" * 60)
        print(f"æ€»æ–‡ç« æ•°: {result['total']}")
        print(f"æˆåŠŸæ›´æ–°: {result['updated']}")
        print(f"è·³è¿‡: {result['skipped']}")
        print(f"å¤±è´¥: {result['errors']}")
        print(f"æ–‡æ¡£æ•°ï¼ˆå«chunkï¼‰: {result['docs_created']}")
        print(f"=" * 60)

        if result['errors'] > 0:
            print(f"\nâš ï¸  {result['errors']} ç¯‡æ–‡ç« æ›´æ–°å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

    except Exception as e:
        print(f"\nâŒ æ‰¹é‡æ›´æ–°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    # 5. æŸ¥çœ‹ç»Ÿè®¡
    print(f"\n4. å‘é‡åº“ç»Ÿè®¡:")
    stats = pipeline.get_stats()
    print(f"   é›†åˆåç§°: {stats.get('collection_name')}")
    print(f"   æ€»æ–‡æ¡£æ•°: {stats.get('total_documents')}")
    print(f"   æ–‡æ¡£ç±»å‹: {stats.get('unique_doc_types')}")
    print(f"   è¯é¢˜æ•°: {len(stats.get('unique_topics', []))}")

    print(f"\nğŸ‰ å®Œæˆï¼ç°åœ¨å¯ä»¥å¯åŠ¨åº”ç”¨äº†:")
    print(f"   ./start_app.sh")


if __name__ == "__main__":
    main()